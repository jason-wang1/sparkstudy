package ml

import org.apache.spark.SparkConf
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.DoubleType
import org.apache.spark.sql.{Column, DataFrame, SparkSession}

/**
  * ç‰¹å¾å·¥ç¨‹+æ¨¡å‹è®­ç»ƒ
  */
object RegressionModelBuild {

  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("RegressionModelBuild").setMaster("local[3]")
    val spark = SparkSession.builder().config(sparkConf).getOrCreate()

    // è¯»å–æ•°æ®
    val basicDF = readData(spark, "/data/df_basic.csv")
    val tradeDF = readData(spark, "/data/df_trade.csv")

    // 1. ç‰¹å¾æ„é€ 
    val basicWithTradeDF = featureBuild(spark, tradeDF, basicDF).cache()

    // 2. ç»Ÿè®¡å„åˆ—çš„ç¼ºå¤±å€¼æ¯”ä¾‹ï¼›ç¼ºå¤±å€¼å¡«å……
    calcNullRate(spark, basicWithTradeDF).show(false)

    val basicFillNullDF = fillNull(spark, basicWithTradeDF).cache()
    basicFillNullDF.show()
    basicFillNullDF.printSchema()

    // 3. ä½¿ç”¨å›å½’æ¨¡å‹è®¡ç®—trade_cnt(x)å¯¹trade_amt(y)çš„å½±å“
    coffBetweenCntAmt(basicFillNullDF)
  }

  /**
    * è¯»å–æ•°æ®
    */
  def readData(spark: SparkSession, relativePath: String) = {
    val basicPath = this.getClass.getResource(relativePath).getPath
    spark.read.option("header", "true").csv(basicPath).cache()
  }

  /**
    * ç‰¹å¾æ„é€ 
    * åœ¨df_basicä¸­ï¼Œæ ¹æ®df_tradeçš„æ•°æ®ä¿¡æ¯ï¼Œè¡ç”Ÿç‰¹å¾â€œæ¯ä¸ªå®¢æˆ·çš„äº¤æ˜“ç¬”æ•°â€å¹¶æ·»åŠ è‡³df_basicä¸­ï¼Œ å‘½åä¸º'trade_cnt'ï¼›
    * åœ¨df_basicä¸­ï¼Œæ ¹æ®df_tradeçš„æ•°æ®ä¿¡æ¯ï¼Œè¡ç”Ÿç‰¹å¾â€œæ¯ä¸ªå®¢æˆ·çš„äº¤æ˜“æ€»é‡‘é¢â€å¹¶æ·»åŠ è‡³df_basicä¸­ï¼Œ å‘½åä¸º'trade_amt';
    */
  def featureBuild(spark: SparkSession, tradeDF: DataFrame, basicDF: DataFrame) = {
    import spark.implicits._
    val tradeAggDF = tradeDF.groupBy($"id")
      .agg(count($"amt").alias("trade_cnt"), sum($"amt").alias("trade_amt"))

    basicDF.join(tradeAggDF, Seq("id"), "left")
      .withColumn("age", $"age".cast(DoubleType))
  }

  /**
    * å°†nullæ›¿æ¢ä¸º1ï¼Œå¦åˆ™æ›¿æ¢ä¸º0ï¼Œç›®çš„æ˜¯ä¸ºäº†ç»Ÿè®¡å„åˆ—çš„ç¼ºå¤±å€¼æ¯”ä¾‹
    */
  def change(colName: Column) ={
    when(isnan(colName) || isnull(colName), 1).otherwise(0)
  }

  /**
    * ç»Ÿè®¡å„åˆ—çš„ç¼ºå¤±å€¼æ¯”ä¾‹
    */
  def calcNullRate(spark: SparkSession, basicWithTradeDF: DataFrame) = {
    import spark.implicits._
    basicWithTradeDF
      .withColumn("gender", change($"gender"))
      .withColumn("edu", change($"edu"))
      .withColumn("age", change($"age"))
      .withColumn("is_vip", change($"is_vip"))
      .withColumn("trade_cnt", change($"trade_cnt"))
      .withColumn("trade_amt", change($"trade_amt"))
      .agg(
        (sum($"gender") / count($"gender")).alias("gender"),
        (sum($"edu") / count($"edu")).alias("edu"),
        (sum($"age") / count($"age")).alias("age"),
        (sum($"is_vip") / count($"is_vip")).alias("is_vip"),
        (sum($"trade_cnt") / count($"trade_cnt")).alias("trade_cnt"),
        (sum($"trade_amt") / count($"trade_amt")).alias("trade_amt")
      )
  }

  /**
    * è®¡ç®—dataFrameæŒ‡å®šåˆ—çš„ä¼—æ•°
    */
  def getModeValue(spark: SparkSession, dataFrame: DataFrame, colName: Column): String ={
    dataFrame
      .na.drop()
      .groupBy(colName)
      .count()
      .rdd
      .map { row => (row.getString(0), row.getLong(1)) }
      .reduce{case ((val1, cnt1), (val2, cnt2)) =>
        if (cnt1 > cnt2) (val1, cnt1)
        else (val2, cnt2)
      }
      ._1
  }

  /**
    * è®¡ç®—dataFrameæŒ‡å®šåˆ—çš„ä¸­ä½æ•°
    */
  def getMedianValue(dataFrame: DataFrame, colName: String): Double={
    val quantiles = dataFrame
      .na.drop()
      .stat
      .approxQuantile(colName, Array(0.5), 0.01)
    quantiles(0)
  }

  /**
    * ç¼ºå¤±å€¼å¡«å……
    * å¯¹trade_cntã€trade_amtçš„ç¼ºå¤±å€¼å¡«å……ä¸º0ï¼›
    * å¯¹å­—ç¬¦å‹ç‰¹å¾çš„ç¼ºå¤±å€¼è¿›è¡Œä¼—æ•°å¡«å……ï¼›
    * å¯¹æ•°å€¼å‹ç‰¹å¾çš„ç¼ºå¤±å€¼è¿›è¡Œä¸­ä½æ•°å¡«å……
    */
  def fillNull(spark: SparkSession, dataFrame: DataFrame) = {
    import spark.implicits._

    val genderMode = getModeValue(spark, dataFrame, $"gender")
    val eduModel = getModeValue(spark, dataFrame, $"edu")
    val isVipMode = getModeValue(spark, dataFrame, $"is_vip")
    val ageMedianValue = getMedianValue(dataFrame, "age")

    dataFrame.na
      .fill(Map("gender" -> genderMode, "edu" -> eduModel,
        "age" -> ageMedianValue, "is_vip" -> isVipMode,
        "trade_cnt" -> 0, "trade_amt" -> 0
      ))
  }

  /**
    * å›å½’æ¨¡å‹
    * è¯·æ„å»ºå›å½’æ¨¡å‹(æ³¨æ˜ï¼šä½¿ç”¨sklearn.linear_modelå†…çš„å‡½æ•°)ï¼Œæ±‚è§£trade_cnt(x)å¯¹trade_amt(y)çš„å½±å“,å¹¶å¯¹è¾“å‡ºç»“æœåšç®€è¦è§£è¯»ï¼›
    *
    * æç¤ºï¼šå‡ºäºå¯¹å¼ºè§£é‡Šæ€§åŠæ¨¡å‹ç²¾åº¦çš„è€ƒè™‘ï¼Œè¯·å°½é‡ä»ç‰¹å¾åŠ å·¥/ç‰¹å¾è¡ç”Ÿç­‰è§’åº¦è¿›è¡Œé¢„å¤„ç†ï¼Œæå‡æ¨¡å‹çš„ ğ‘…2
    */
  def coffBetweenCntAmt(basicFillNullDF: DataFrame) = {
    val assembler = new VectorAssembler()
      .setInputCols(Array("trade_cnt"))
      .setOutputCol("features")

    val lr: LogisticRegression = new LogisticRegression()
      .setMaxIter(20)
      .setRegParam(0.01)
      .setFeaturesCol("features")
      .setLabelCol("trade_amt")

    val pipeline = new Pipeline()
      .setStages(Array(assembler, lr))

    val model: PipelineModel = pipeline.fit(basicFillNullDF)

    val lrModel = model.stages(1).asInstanceOf[LogisticRegressionModel]
    println(s"Coefficients: ${lrModel.coefficientMatrix}  Intercept: ${lrModel.interceptVector}")
  }
}
